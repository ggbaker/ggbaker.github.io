[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a dissertator in the Department of Economics at the University of Wisconsin\u0026ndash;Madison. I\u0026rsquo;m an economic theorist interested in information economics and decision theory. My current research studies the demand and valuation of information.\nI am currently on the job market and will be available for remote interviews by the usual means (Skype, Zoom, etc.) including during the virtual European Job Market.\n","date":1603152000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1603152000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.garygbaker.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a dissertator in the Department of Economics at the University of Wisconsin\u0026ndash;Madison. I\u0026rsquo;m an economic theorist interested in information economics and decision theory. My current research studies the demand and valuation of information.\nI am currently on the job market and will be available for remote interviews by the usual means (Skype, Zoom, etc.) including during the virtual European Job Market.","tags":null,"title":"Gary Baker","type":"authors"},{"authors":["Gary Baker"],"categories":[],"content":"    A scene from my department job talk   Background Like most grad students entering the job market this year, I\u0026rsquo;ve had to wrestle with the transition to giving talks remotely. Personally, I have some trouble focusing on virtual seminars since the format tends to be fairly static: mostly still slides with perhaps a camera window to the side. To avoid this, I\u0026rsquo;ve spent a fair amount of time (read: procrastinating) trying to put together a setup that allows me to move around and interact with my materials a bit more. Below, I outline an approach that\u0026rsquo;s worked reasonably well for me.\nSince I\u0026rsquo;m not teaching this semester, I\u0026rsquo;ve mostly focused on how to adapt an academic seminar to a remote setting, but I would use a variation of this for remote teaching given the opportunity.\nI set out to replicate three particular aspects of in-person seminars/teaching:\n The ability to easily handwrite equations/graphs on a blackboard, The ability to draw attention to certain parts of a slide or blackboard, whether by laser pointer or physically pointing, and The ability to move and gesture while speaking.  For all of this, I\u0026rsquo;m indebted to the work of others who took time to experiment with different methods for remote teaching/seminars and create guides. In particular,\n Luke Stein\u0026rsquo;s YouTube series: OBS for teaching tutorial David J. Malan\u0026rsquo;s post: Teaching from Home via Zoom A variety of twitter posts by Emily Nix, for example, here and here  Hardware Needed harware:  A computer (obviously): a relatively recent Mac would work best, but isn\u0026rsquo;t required (My main desktop runs Linux) A good camera A tablet with stylus (iPad with Apple Pencil) A second monitor is a big plus  I can\u0026rsquo;t overstate the importance of a good camera. If you have a reasonably modern digital camera, there\u0026rsquo;s a good chance it can send a video feed over usb to a computer. I use a Nikon D5500 that I normally use for bird photography, but you almost certainly have a pretty good webcam already available to you: a smart phone. Prior to rigging up my Nikon, I simply used my smartphone on a cheap gooseneck clamp using a free app called Droidcam. A wider angle lens is a big plus here to give yourself more room to move around without having to stand too far back.\nEven with a good camera, you need to be extremely well lit to get a good image (and to get a good green screen effect). I use three lamps each with some cheap 1500 lumen LED bulbs.\nFor the microphone, so long as you don\u0026rsquo;t use your laptop\u0026rsquo;s built in-mic, I don\u0026rsquo;t think the microphone is that important. The inline mic in most earbuds works reasonably well, but it might be worth spending a little bit on the mic. I use a Blue Yeti which works well enough (as a condenser mic, it works reasonably well even sitting a bit back from it), but if I had to do it again, I probably would have gotten a cheap lav mic.\nOf course, to put yourself in the same scene with slides, there\u0026rsquo;s not much substitute for a proper green screen. I got a cheap one on Amazon for under $20, but be aware that the cheap screens come folded and need steam-ironing to remove the creases. Ideally, you\u0026rsquo;ll have a wall behind you to mount it, but otherwise you\u0026rsquo;ll need some kind of stand (which will add some expense).\nLastly, you\u0026rsquo;ll need some way to write easily. For me, that means an iPad with the Apple Pencil. You will also need some way to get the video feed from the tablet into your computer.\nFinally, it\u0026rsquo;s extremely useful (though not necessary) to have at least one extra monitor so you can have the chat window and participants\u0026rsquo; videos visible separately.\nRoom setup    A very messy desk set up for presenting   I sit on a bar stool about 2 meters back from my camera in the left third of the camera\u0026rsquo;s field of view. This leaves most of the camera\u0026rsquo;s field of view as empty green screen that I can fill with the slides and gesture over. In the image below showing the OBS setup, the red box is the extent of the area I can gesture in.\nSince I\u0026rsquo;m sitting so far back from my desk, I also use a small table for my iPad, and I adjust my microphone to be as close as possible.\n   Where I sit   Software Needed software  OBS Virtual Cam Extension for OBS (Windows, Mac, Linux) PDF software of choice on for the tablet (I use Readdle Documents on the iPad) Some way to get the video feed from the tablet to the computer (see below)  OBS setup    OBS Scene arrangement   Since many others have given excellent tutorials on how to use OBS (see Luke Stein\u0026rsquo;s tutorial linked above), I will assume some familiarity with it.\nMy primary scene in OBS has three sources:\n Webcam input, with Chroma Key (green screen) filter applied, and with the image reflected (so I can more easily see where I\u0026rsquo;m \u0026ldquo;pointing\u0026rdquo;) Flat color as a background (black worked best for me) Video input from my iPad (see below)  In order to make better use of the space in the OBS canvas, I also adjusted the aspect ratio of my beamer slides to be a bit more square (12x10 worked will for me). This can be accomplished using the beamerposter package:\n\\usepackage[orientation=landscape,size=custom,width=12,height=10,scale=0.5,debug]{beamerposter} Video input from tablet The biggest challenge of all this was finding a way to mirror my iPad\u0026rsquo;s display to my desktop so OBS could use it. If you have a relatively recent Mac, this is easy since recent Macs and iPads by default support a system called Sidecar that allows you to use your iPad as a second screen (with touch support) for your Mac.\nSince my main desktop isn\u0026rsquo;t a Mac, I settled on using Apple\u0026rsquo;s Airplay protocol to wirelessly mirror my iPad display to my desktop; however, this isn\u0026rsquo;t supported by default (Airplay is meant to mirror to a device like an AppleTV). Fortunately, with appropriate software, a regular computer can become an Airplay receiver. On Linux, this can be accomplished with an open source tool called UxPlay. I know similar software exists for Windows, but all options I saw were paid software, so I\u0026rsquo;m hesitant to recommend one in particular without trying them first.\nZoom setup Finally, we just need to get everything into Zoom. There\u0026rsquo;s two ways to do this, each with it\u0026rsquo;s own advantages.\nWebcam The most straightforward method is to simply use the OBS virtual cam extension (link above). Within Zoom, I\u0026rsquo;d recommend specifically \u0026ldquo;spotlighting\u0026rdquo; your camera so the view doesn\u0026rsquo;t switch to another person\u0026rsquo;s camera when they speak.\nBesides simplicity, this method is pretty much sure to work even if you\u0026rsquo;re not using Zoom (say, Skype or Blackboard Collaborate)\u0026mdash;so long as whatever system you\u0026rsquo;re using supports a webcam.\nThe main issue I had with this method is that Zoom\u0026rsquo;s compression can make the slide text difficult to read at times unless the internet connection for everyone is absolutely perfect.\nThis method also has the advantage that it\u0026rsquo;s likely to work with other software such as Skype or Blackboard Collaborate.\nScreen sharing I eventually settled on using the Screen Share feature with the \u0026ldquo;optimize for video\u0026rdquo; setting enabled.\nFirst, I create a window with the OBS canvas output (right click on the OBS canvas -\u0026gt; Fullscreen projector). I then share this window with Zoom. Once screen sharing, I then disable my webcam in zoom to avoid duplicate images.\nThis method mostly fixes the blurry text issue. Unfortunately, the video feed can lag the audio a bit which can be distracting for the viewers. This lag is somewhat unpredictable; in one of my practice talks it was apparently up to second, and in my final department talk it wasn\u0026rsquo;t much of an issue at all. Your mileage may vary.\nI haven\u0026rsquo;t tested this method with non-Zoom conference software, but I wouldn\u0026rsquo;t expect it to work well there. It seems like most software uses a compression optimized for relatively static scenes for screen sharing, so movement would likely appear very choppy to viewers. It only works in Zoom because of Zoom\u0026rsquo;s \u0026ldquo;optimize for video option\u0026rdquo; for screen sharing.\nThe final result    The final result   Overall, the whole setup is a bit of a house of cards, but it mostly replicated what I felt were the most important aspects of in-person talks.\nAlthough I haven\u0026rsquo;t had a chance to use this for teaching yet, I expect things would work similarly well, either using a note-taking app on the iPad for a pure \u0026ldquo;chalk-and-talk\u0026rdquo; sort of lecture or using a hybrid approach mixing slides with handwriting.\n","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"79019f34da5aa63260caee4a20955cbd","permalink":"https://www.garygbaker.com/post/presentations/","publishdate":"2020-10-20T00:00:00Z","relpermalink":"/post/presentations/","section":"post","summary":"Like most grad students entering the job market this year, I've had to wrestle with the transition to giving talks remotely. Personally, I have some trouble focusing on virtual seminars since the format tends to be fairly static: mostly still slides with perhaps a camera window to the side. To avoid this, I've spent a fair amount of time (*read*: procrastinating) trying to put together a setup that allows me to move around and interact with my materials a bit more. Here, I outline an approach that's worked reasonably well for me.","tags":[],"title":"Remote teaching and seminars with OBS","type":"post"},{"authors":null,"categories":null,"content":"","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"ad0a3cd315d63f1e0460cf310e351f3b","permalink":"https://www.garygbaker.com/publication/info-consumer-theory/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/info-consumer-theory/","section":"publication","summary":"My job market paper examines a scenario where a decision-maker trying to learn about an underlying state of the world must decide not only *how much* information to purchase, but also *from which* sources. I develop an approximate consumer theory for information, valid when information purchases are large and the relevant probabilities are well-described by large-deviations theory.","tags":null,"title":"Consumer theory for cheap information","type":"publication"},{"authors":null,"categories":null,"content":"My job market paper considers the substitutability between different information sources at large sample size\u0026mdash;that is, when information is sufficiently cheap and/or budgets sufficiently large. This file illustrates the main results using Python.\nIf you\u0026rsquo;d like to experiment yourself, the original Jupyter notebook can be downloaded here (Right click -\u0026gt; \u0026ldquo;Save Link As\u0026rdquo;).\nCode setup First, we need to import a number of standard Python packages:\nimport numpy as np # Basic array stuff import scipy.optimize as optim # For finding function mins from scipy.stats import multinomial # Multinomial probability computation import matplotlib.pyplot as plt # Plotting from tabulate import tabulate # For nicer printing of some data Model A decision maker must take one of finitely many actions $a\\in A$ facing an uncertain state of the world, $\\theta$, that is one of finitely many possible states. She has a state-dependent payoff function, $u(a,\\theta)$ and chooses her action to maximize expected payoff. Her prior is given by $p\\in\\Delta\\Theta$.\nPrior to acting, she may purchase information about the state and update her prior based on that information. As standard, define an information as a Blackwell experiment, that is, a collection of state-dependent distributions, $F(r\\ |\\ \\theta)$ over some realization space, R:\n$$ \\mathcal{E} \\equiv \\{R, \\langle F(\\cdot\\ |\\ \\theta)\\rangle_{\\theta\\in\\Theta}\\} $$\n(In a fully formal treatment, the definition would also include a σ-algebra. For the purposes of this paper, we can ignore such measure-theoretic complications).\nAfter observing a realization from an information source, the decision maker can update with Bayes rule:\n$$ p_\\theta\u0026rsquo;(r) = \\frac{p_\\theta f(r\\ | \\ \\theta)}{\\sum_{\\theta\u0026rsquo;\\in\\Theta}p_{\\theta\u0026rsquo;}f(r\\ |\\ \\theta\u0026rsquo;)} $$\nTo avoid trivialities, assume that no realization perfectly rules in or out any subset of the states, that is, if realization has positive probability (density) under one state, it must have positive probability under all states. (In technical terms, assume the $F(\\cdot\\ |\\ \\theta)$ are all mutually absolutely continuous so the Radon-Nikodym derivatives, $dF(\\cdot\\ |\\ \\theta\u0026rsquo;)/dF(\\cdot\\ |\\ \\theta)$ all exist.) For notational simplicity in this illustration, I\u0026rsquo;ll assume each state-dependent distribution has finitely many possible realizations and thus pmf given by, $f$.\nWe can the define an amount of information by a number of conditionally independent samples from such a source.\nFor illustration, consider a two-state world, $\\theta\\in\\{H,L\\}$. An information source might be a coin that is fairly waited in the $L$ state, and biased 70% to heads in the $H$ state. Then samples from this source would simply be the number of coin flips. In an experimental setting, samples would be literal samples under some experimental design.\nThe DM has a collection of information sources $\\mathcal{E}_i,\\ldots, \\mathcal{E}_I$ from each of which she can purchase an arbitrary number of samples, $\\mathbf{n}=[n_i]$, at some cost $[c_i]$ each.\nThe goal of this paper is to characterize the substitutability of different information sources under the normal Bayesian (ex ante) information value\u0026mdash;that is, the expected payoff gain from acting after observing a realization from information source $\\mathcal{E}$:\n$$ V(\\mathcal{E}) \\equiv \\sum_{r\\in R} \\max_a {\\sum_\\theta p\u0026rsquo;_\\theta(r) u(a,\\theta)} f(r) - \\max_a {\\sum_\\theta p_\\theta u(a,\\theta)} $$ where $f(r)\\equiv \\sum_\\theta p_\\theta f(r\\ |\\ \\theta)$ is the unconditional realization probability for the given source.\nInformation value is typically a very poorly behaved function, so I approach the problem with an asymptotic approach using large deviations methods.\nThroughout this notebook, I\u0026rsquo;ll be working with a 3 state decision problem.\nnumstates = 3 Composite information sources and information values Defining information sources In order to simulate information values, we need a way to define Blackwell experiments in a way amenable to computation: Define an information source as a $|\\Theta|\\times|R|$ matrix, so each row of the matrix lists the probability of each realization in that state. I will typically use Q to denote such a matrix.\nThe following code will specify a pair of experiments that happens to look nice, but if you\u0026rsquo;re running the code yourself, you can use the upper lines to create a pair of random experiments.\ndef rand_source(numstates, numrealizations): # generate a random array with the appropriate dimension Q = np.random.rand(numstates, numrealizations) # normalize so each row sums to 1 for state in range(numstates): Q[state, :] = Q[state, :] / np.sum(Q[state, :]) return Q #Q1 = rand_source(numstates, 2) #Q2 = rand_source(numstates, 3) Qperfect = np.eye(numstates) # Perfect information source # The following two make nice plots Q1 = np.array([[0.42, 0.58], [0.63, 0.37], [0.03, 0.97]]) Q2 = np.array([[0.07, 0.18, 0.75], [0.45, 0.19, 0.36], [0.45, 0.05, 0.50]]) # state labels. Will use for nice output tables states = [] for stateidx in range(Q1.shape[0]): states.append([\u0026#34;State {i}\u0026#34;.format(i=stateidx)]) print(\u0026#34;Conditional probability of each realization:\\n\u0026#34;) # Output table and print table1 = np.append(states, Q1, axis=1) realizations1 = [\u0026#34;Q1\u0026#34;] for realization in range(Q1.shape[1]): realizations1.append(\u0026#34;R1 =\u0026#34; + \u0026#34; \u0026#34; + str(realization)) print(tabulate(table1, headers=realizations1)+\u0026#34;\\n\u0026#34;) table2 = np.append(states, Q2, axis=1) realizations2 = [\u0026#34;Q2\u0026#34;] for realization in range(Q2.shape[1]): realizations2.append(\u0026#34;R2 =\u0026#34; + \u0026#34; \u0026#34; + str(realization)) print(tabulate(table2, headers=realizations2)) Conditional probability of each realization: Q1 R1 = 0 R1 = 1 ------- -------- -------- State 0 0.42 0.58 State 1 0.63 0.37 State 2 0.03 0.97 Q2 R2 = 0 R2 = 1 R2 = 2 ------- -------- -------- -------- State 0 0.07 0.18 0.75 State 1 0.45 0.19 0.36 State 2 0.45 0.05 0.5  Now, we need to be able to quickly compute the matrix for composite experiments. First, we need to be able to compute the matrix for $n$ i.i.d. samples from 1 experiment.\nSince the total number of realizations of each type is a sufficient statistic for the entire vector of realizations, we can simplify things by first computing all of the partitions of $n$ with $|R|$ components (all possible realization sums), the use a multinomial distribution.\nThe output matrix will be $|\\Theta|\\times$(number of ways to sum $|R|$ positive integers to add up to $n$)\n# Compute appropriate partitions (returns a generator) def partitions(n, numrealizations): if numrealizations == 1: if n \u0026gt;= 0: yield (n,) return for i in range(n+1): for result in partitions(n-i, numrealizations-1): yield (i,) + result # Compute matrix for the n-sample source def n_samples(Q, n): numstates = Q.shape[0] numrealizations = Q.shape[1] if n == 0: # return trivial experiment if 0 samples return np.ones((numstates, 1)) QnT = [] # transpose of Qn for outcome in partitions(n, numrealizations): outcomeprobs = [] # column of state-dep outcome probs for state_idx in range(numstates): # create a multinomial with the given outcome probs multinom = multinomial(n, Q[state_idx, :]) outcomeprobs.append(multinom.pmf(outcome)) QnT.append(outcomeprobs) Qn = np.array(QnT).T # convert to array and transpose return Qn n = 3 realizations1 = [str(n) + \u0026#34; samples of Q1\u0026#34;] for outcome in partitions(n, Q1.shape[1]): realizations1.append(outcome) table = np.append(states, n_samples(Q1, n), axis=1) print(\u0026#34;conditional probabilities of each sample combination:\\n\u0026#34;) print(tabulate(table, headers=realizations1)) conditional probabilities of each sample combination: 3 samples of Q1 (0, 3) (1, 2) (2, 1) (3, 0) ----------------- -------- -------- -------- -------- State 0 0.195112 0.423864 0.306936 0.074088 State 1 0.050653 0.258741 0.440559 0.250047 State 2 0.912673 0.084681 0.002619 2.7e-05  Second, we need to be able to composite two distinct information sources. If info source $\\mathcal{E}_1$ and $\\mathcal{E}_2$ have $|R_1|$ and $|R_2|$ possible realizations respectively, then the composite source consisting of 1 sample from each has $|R_1|\\times|R_2|$ outcomes. We can get a matrix of all possible combination probabilities by simply by listing out each element of the outer product of the rows of each matrix:\ndef composite_source(Q1, Q2): numstates = Q1.shape[0] numrealizations = Q1.shape[1] * Q2.shape[1] Qcomp = np.empty((numstates, numrealizations)) # initialize output for state in range(numstates): # compute all possible combination probs with an outer product Qcomp[state, :] = \\ np.reshape(np.outer(Q1[state, :], Q2[state, :]), (numrealizations)) # reshape to vect. return Qcomp Q12comp = composite_source(Q1, Q2) realizations12 = [\u0026#34;1 from each\u0026#34;] for r1idx in range(Q1.shape[1]): for r2idx in range(Q2.shape[1]): realizations12.append(\u0026#34;R1={i1}, R2={i2}\u0026#34;.format(i1=r1idx, i2=r2idx)) table = np.append(states, Q12comp, axis=1) print(\u0026#34;Conditional probabilties of each combination of realizations from Q1 and Q2:\\n\u0026#34;) print(tabulate(table, headers=realizations12)) Conditional probabilties of each combination of realizations from Q1 and Q2: 1 from each R1=0, R2=0 R1=0, R2=1 R1=0, R2=2 R1=1, R2=0 R1=1, R2=1 R1=1, R2=2 ------------- ------------ ------------ ------------ ------------ ------------ ------------ State 0 0.0294 0.0756 0.315 0.0406 0.1044 0.435 State 1 0.2835 0.1197 0.2268 0.1665 0.0703 0.1332 State 2 0.0135 0.0015 0.015 0.4365 0.0485 0.485  Note that we repeated composite a matrix with itself to get an equivalent $n$ sample matrix, but this would produce a massive matrix (size $|R|^n$). Most computers would hit memory limitations for any $n$ bigger than 20 or so brute forcing it like that isn\u0026rsquo;t practical.\nValue of information In order to compute information value, we must now define a state-dependent utility function and a prior belief. I\u0026rsquo;ll code the utility function as a $|A|\\times|\\Theta|$ matrix of payoffs where $U_{a\\theta}=u(a,\\theta)$. The prior can simply be coded as a vector of belief probabilities.\n# example payoff matrix (payoff 1 only if choose the correct state) U = np.eye(numstates) # example prior vector (diffuse prior) P = np.ones(numstates) / numstates Information value is typically a fairly tricky thing to compute. In order to maximize computational efficiency, I vectorize the problem where possible. For a given information matrix, $Q$, and payoff matrix $U$, we can write the value with information as\n$$ W(Q) = \\max_D{\\text{tr}(QDU\\pi)}$$\nwhere $\\pi$ is a matrix who\u0026rsquo;s diagonal elements are the prior probabilities and $D$ is a $|R|\\times|A|$ matrix specifying the probability of taking each action after each realization (this is a linear program: $D$ generically is all zeros and ones since each realization generically has a unique optimal response.\n(Leshno, 1992 uses this formulation to provide an elementary proof of Blackwell\u0026rsquo;s theorem for the finite-action/finite-state case.)\nThe value of information would then be $W(Q)$ minus the payoff from acting with no information. Such a subtraction is a monotone transformation, so it won\u0026rsquo;t affect the ordinal properties I\u0026rsquo;m interested in.\nIn order to evaluate how close a bundle is to perfect information, I will sometimes use the ratio of the full-information gap (the difference between the value of a perfect signal and $W(Q)$, relative to the full-info value. This will be a percentage that approaches zero as the amount of samples increases.\nNone of these approximation would be particularly useful if they require so many samples as to be indistinguishable from a perfect source anyways. I will thus use this relative info-gap as an ad hoc measure how useful the approximation is. That is, the relevant approximations are useful if they are accurate, even when the relative info-gap is large.\ndef info_value(Q, U, P): numrealizations = Q.shape[1] Upi = U @ np.diag(P) # compute (actions x realizations) matrix of payoff of each action # times unconditional prob of each realization Ua = Upi @ Q # choose best action for each message then sum across messages valuewithinfo = sum(np.amax(Ua, axis=0)) return valuewithinfo # relative info gap def relative_info_gap(Q, U, P): return (info_value(Qperfect, U, P) - info_value(Q, U, P)) / info_value(Qperfect, U, P) # value of info for the examples above print(tabulate([[\u0026#34;Expected value of acting after observing Q1:\u0026#34;, info_value(Q1, U, P)]])) -------------------------------------------- -------- Expected value of acting after observing Q1: 0.533333 -------------------------------------------- --------  Chernoff precision In the paper, I show that two information sources are exchangeable with ratios of respective precision-like indices of each experiment. In order to define this precision, we must first take a brief detour into large deviations theory.\nI approach the problem of approximating information values by approximating the probability of a \u0026ldquo;mistake\u0026rdquo; (taking a suboptimal action in a given state). The normal form of Bayes\u0026rsquo;s rule is a bit messy, so instead of working with probabilities, I work with log-likelihood ratios, where Bayes rule becomes a sum:\n$$ \\log\\bigg(\\frac{p_{\\theta}'(r)}{p_{\\theta\u0026rsquo;}'(r)}\\bigg) = \\log\\bigg(\\frac{p_\\theta}{p_{\\theta\u0026rsquo;}}\\bigg) + \\log\\bigg(\\frac{f(r\\ |\\ \\theta)}{f(r\\ |\\ \\theta)}\\bigg) $$\nAnd, of course, we have no shortage of asymptotic results for approximating sums of many independent distributions.\nFor a given pair of states, define the Chernoff index of an experiment, $\\mathcal{E}_1$, as the minimized value of the moment generating function (MGF) of the distribution of log-likelihood ratios (LLR):\n$$ \\rho_1 \\equiv \\min_t \\sum_r f(r\\ |\\ \\theta)^t f(r\\ |\\ \\theta\u0026rsquo;)^{1-t} $$ (Define $\\tau_1$ as the minimizer)\nNote that the expected value of the distribution of the above mgf is the negative Kullback-Leibler divergence, $-D(F(\\cdot\\ |\\ \\theta\u0026rsquo;)\\ ||\nF(\\cdot\\ |\\ \\theta))$.\nNote that because the MGF of an independent sum is the product of MGFs, we have that $n$ samples from $\\mathcal{E}_1$ will have Chernoff index $\\rho_1^n$.\nFurthermore, because the minimum of a sum will be bigger than the sum of minima, we have that the Chernoff index of a composite is more than the sum of its parts:\n$$ \\rho_{12} \\geq \\rho_1\\rho_2 $$\nNow, we can define the Chernoff precision for a given state pair of a test by $\\beta \\equiv -\\log(\\rho)$. I call this a precision because, for Gaussian tests, it is, up to a multiplicative constant, the same as classical precision ($1/\\sigma^2$).\nThis measure has a number of properties that you might expect for something called a precision\n For any non-trivial experiment, $\\beta\u0026gt;0$ $n$ samples from the same experiment has precision $n\\beta$ Blackwell dominant experiments have higher precision  Intuitively, you can think of the Chernoff precision as measuring how well an information source can distinguish between a given pair of states.\nBecause the Chernoff number of a composite is weakly higher than the product of the individual Chernoff numbers, a composite experiment is weakly less precise, for a given state, than the sum of it\u0026rsquo;s parts.\n# the following returns the full list of state-pair precisions # and their respective MGF minimizers def precisions(Q): numstates = Q.shape[0] # compute the Chernoff index for each state betalist = [] taulist = [] for state1 in range(numstates): for state2 in range(state1+1, numstates): # Define the llr mgf for the given dichtomy def llrmgf(t): Qstate1t = Q[state1, :]**t Qstate2t = Q[state2, :]**(1-t) return np.sum(Qstate1t * Qstate2t) # Compute index for the dichotomy optimizer = optim.minimize(llrmgf, 0.5) rho = optimizer.fun tau = optimizer.x[0] betalist.append(-np.log(rho)) taulist.append(tau) return betalist, taulist beta1list, tau1list = precisions(Q1) beta2list, tau2list = precisions(Q2) beta12list, tau12list = precisions(composite_source(Q1, Q2)) table = np.array([beta12list, [beta1list[i]+beta2list[i] for i in range(len(beta1list))]]).T print(tabulate(table, headers=[\u0026#34;composite precisions\u0026#34;, \u0026#34;sum of parts\u0026#34;]))  composite precisions sum of parts ---------------------- -------------- 0.149026 0.149196 0.271436 0.276094 0.343391 0.343559  Now, it might seem that composites are always worse than the sum of their parts since, for any state pair, the composite is always less precise than the sum of its parts. But Moscarini and Smith (2002) showed that, for large sample sizes, the only state pair that matters is the pair hardest to tell apart\u0026mdash;i.e. the pair with the least precision (highest Chernoff index).\nThus complementarity often arises when experiments differ in the pair of states they most struggle to distinguish:\nbeta1 = min(beta1list) beta2 = min(beta2list) beta12 = min(beta12list) print(tabulate([[beta12, beta1+beta2]], headers=[\u0026#34;min precision of composite\u0026#34;, \u0026#34;sum of min precisions\u0026#34;]))  min precision of composite sum of min precisions ---------------------------- ----------------------- 0.149026 0.051485  In this particular example, we can see that it is in fact the composite has a higher least precision than the sum of least precisions of its parts.\nPlotting \u0026ldquo;indifference curves\u0026rdquo; In particular, at large samples, two bundles of samples will perform equally if they have equal least precision. Information value at large samples is ordinally equivalent to\n$$ v(n_1, n_2) \\simeq (n_1+n_2)\\beta_\\omega $$\nwhere $\\beta_\\omega$ is the least precision dichotomy for a bundle that is composed of $\\omega$ fraction of samples from $n_1$. Furthermore, we can breakdown $\\beta_\\omega$ into component precisions:\n$$ \\beta_\\omega=\\omega\\beta_{\\omega 1}+(1-\\omega)\\beta_{\\omega 2} $$\nwhere $\\beta_{\\omega i}$ is $-\\log M_i(\\tau_\\omega)$, is the negative log of the LLR MGF for the composite\u0026rsquo;s worst-case state pair, evaluated at the composite\u0026rsquo;s minimizer. We can then write\n$$ v(n_1, n_2) \\simeq n_1\\beta_{\\omega 1} + n_2\\beta_{\\omega 2} $$\nHeuristically, then it seems like the MRS between two samples at any bundle with $\\omega$ fraction from $\\mathcal{E}_1$ must then be the ratio of the component precisions. (For small substitutions, relative to total sample size, the fraction of samples from each source doesn\u0026rsquo;t change much, so the component precisions don\u0026rsquo;t change much.)\nAdditionally, since the value is a min of a sums of precisions, there will be kinks when the least-precision state pair changes.\nOf course, samples are fundamentally discrete so there is no MRS. In the paper, I formally define a notion of asymptotic MRS, which basically defines the slope of the boundary between upper and lower contour sets. For the purpose of interpreting things here, it works well enough to just pretend samples are divisible.\nFirst, note that the component precisions only depend on the fraction of the bundle from each source (info value is homothetic). First, I compute the component precision for a given composite factor $\\omega$.\nn1start = 50 # return the component precision for each test at the w composite factor def comp_precision(Q1, Q2, w): numstates = Q1.shape[0] # loop over pairs of states rho = 0 for state1 in range(numstates): for state2 in range(state1+1, numstates): # Define the Hellinger transform for the given dichtomy def llrmgf1(t): Q1state1_t = Q1[state1, :]**t Q1state2_t = Q1[state2, :]**(1-t) return np.sum(Q1state1_t*Q1state2_t) def llrmgf2(t): Q2state1_t = Q2[state1, :]**t Q2state2_t = Q2[state2, :]**(1-t) return np.sum(Q2state1_t*Q2state2_t) def compllrmgf(t): return llrmgf1(t)**w * llrmgf2(t)**(1-w) # Compute Chernoff for the dichotomy optimizer = optim.minimize(compllrmgf, 0.5) rhopair = optimizer.fun taupair = optimizer.x # if new rho is worse (higher), store it if rho \u0026lt; rhopair: rho = rhopair # Store component rhos rho1w = llrmgf1(taupair) rho2w = llrmgf2(taupair) return -np.log(rho1w), -np.log(rho2w) # return total precision for a composite factor w def total_precision(Q1, Q2, w): beta1w, beta2w = comp_precision(Q1, Q2, w) return w*beta1w + (1-w)*beta2w We can then compute points on an indifference curve by using the differential equation defined by the asymptotic MRS ($dn_2/dn_1$):\n$$ \\text{AMRS}(\\omega) = \\frac{\\beta_{\\omega 1}}{\\beta_{\\omega 2}} $$\nIn all the plots that follow, the reference point is the lower right corner bundle consisting entirely of samples from $\\mathcal{E}_1$.\ndef mrs_approx(n1start): dn = 0.01 n1pointsapprox = np.arange(n1start, -dn, -dn) n2pointsapprox = [] n2 = 0 for n1 in n1pointsapprox: w = n1 / (n1 + n2) beta1w, beta2w = comp_precision(Q1, Q2, w) n2 = n2 + dn*(beta1w/beta2w) n2pointsapprox.append(n2) return n1pointsapprox, n2pointsapprox n1pointsapprox, n2pointsapprox = mrs_approx(n1start) Now we can compare this to the true upper/lower contour set computed numerically using the info value function defined earlier. The plot below shows the locus of bundles that are minimally better than the reference point (the maximal boundary for the upper contour set).\ndef mrs_true(n1start): Q1n = n_samples(Q1, n1start) startval = info_value(Q1n, U, P) # Trace out the lower extent of the UCS n2 = 0 # start with no samples from Q2 n1pointstrue = np.arange(n1start, 0, -1) n2pointstrue = [] for n1loss in range(1, n1start+1): Q1n = n_samples(Q1, n1start-n1loss) # find minimum samples from Q2 to make better off currentval = 0 while currentval \u0026lt; startval: n2 = n2 + 1 Q2n = n_samples(Q2, n2) Qcomp = composite_source(Q1n, Q2n) currentval = info_value(Qcomp, U, P) n2pointstrue.append(n2) n2 = n2 - 1 # decr. n2 just to be sure later return n1pointstrue, n2pointstrue n1pointstrue, n2pointstrue = mrs_true(n1start) fig = plt.figure(figsize=(7, 7), dpi=1000) ax = fig.add_subplot(1,1,1) ax.set_xlim((0, n1start)) ax.set_ylim((0, max(n2pointstrue))) ax.set_xlabel(\u0026#39;Number of samples from Experiment 1\u0026#39;) ax.set_ylabel(\u0026#39;Number of samples from Experiment 2\u0026#39;) ax.plot(n1pointstrue, n2pointstrue, \u0026#39;k\u0026#39;, n1pointsapprox, n2pointsapprox, \u0026#39;--k\u0026#39;); Now compute the relative info gap for the indifference curve plotted above. Higher relative info-gap implies the approximation is useful even at small samples.\nQ1n = n_samples(Q1, n1start) print(tabulate([[\u0026#34;Relative info-gap:\u0026#34;, relative_info_gap(Q1n, U, P)]])) ------------------ --------- Relative info-gap: 0.0438343 ------------------ ---------  Note that the approximation effectively sets the probability of a mistake other than the most likely one to zero. This approximation then tends to overestimate payoffs, and thus the approximate MRS will tend to lie to left of the truth.\nIn the above plot, we can see that approximation performs relatively well, even at small sample sizes. One limitation is that the approximation will always perform somewhat poorly in a region around a kink for two reasons:\n The second lowest precision is very close to the lowest, so only accounting for the lowest precision doesn\u0026rsquo;t work as well; and, Because the kinks are inward pointing, total sample size tends to be lower there. In the above example, the corners have total sample size between 40 and 50, but the kink has only about 15 total samples.  But regardless, as sample size increases, we can always get an arbitrarily good approximation for composite factors arbitrarily close to that of any kink point.\nBelow I plot again, but at twice the sample size to illustrate the convergence:\nn1start = 100 n1pointsapprox, n2pointsapprox = mrs_approx(n1start) n1pointstrue, n2pointstrue = mrs_true(n1start) fig = plt.figure(figsize=(7, 7), dpi=500) ax = fig.add_subplot(1,1,1) ax.set_xlim((0, n1start)) ax.set_ylim((0, max(n2pointstrue))) ax.set_xlabel(\u0026#39;Number of samples from Experiment 1\u0026#39;) ax.set_ylabel(\u0026#39;Number of samples from Experiment 2\u0026#39;) ax.plot(n1pointstrue, n2pointstrue, \u0026#39;k\u0026#39;, n1pointsapprox, n2pointsapprox, \u0026#39;--k\u0026#39;); Q1n = n_samples(Q1, n1start) print(tabulate([[\u0026#34;Relative info-gap:\u0026#34;, relative_info_gap(Q1n, U, P)]])) ------------------ -------- Relative info-gap: 0.011011 ------------------ --------  ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"8758b7926efa629fc1aaba272a82309c","permalink":"https://www.garygbaker.com/web-appendices/info-consumer-theory/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/web-appendices/info-consumer-theory/","section":"web-appendices","summary":"My job market paper considers the substitutability between different information sources at large sample size\u0026mdash;that is, when information is sufficiently cheap and/or budgets sufficiently large. This file illustrates the main results using Python.\nIf you\u0026rsquo;d like to experiment yourself, the original Jupyter notebook can be downloaded here (Right click -\u0026gt; \u0026ldquo;Save Link As\u0026rdquo;).\nCode setup First, we need to import a number of standard Python packages:\nimport numpy as np # Basic array stuff import scipy.","tags":null,"title":"Consumer theory for cheap information - Simulations","type":"web-appendices"},{"authors":null,"categories":null,"content":"Bayesian foundations for an asymptotically optimal experiment design, with Sam Engle\nOptimal fast experimentation, with Lones Smith\nBargaining with multiple buyers: evidence from eBay\n","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"51a4fee06c37f65584d0542b60a13baa","permalink":"https://www.garygbaker.com/publication/in-progress/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/in-progress/","section":"publication","summary":"*Bayesian foundations for an asymptotically optimal experiment design,* with Sam Engle\\\n*Optimal fast experimentation,* with Lones Smith\\\n*Bargaining with multiple buyers: evidence from eBay* ","tags":["Information","Decision theory"],"title":"In progress research","type":"publication"},{"authors":null,"categories":null,"content":"  Spring 2018 - Economics 101 - Principles of Microeconomics (Head TA)\n Fall 2017 - Economics 101 - Principles of Microeconomics (Head TA)\n Fall 2015 - Economics 102 - Principles of Macroeconomics  (Head TA)  Spring 2015 - Economics 101 - Principles of Microeconomics \n Fall 2014 - Economics 101 - Principles of Macroeconomics   ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"fad585a23713d04eb8a538204a269145","permalink":"https://www.garygbaker.com/teaching/past/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/teaching/past/","section":"teaching","summary":"  Spring 2018 - Economics 101 - Principles of Microeconomics (Head TA)\n Fall 2017 - Economics 101 - Principles of Microeconomics (Head TA)\n Fall 2015 - Economics 102 - Principles of Macroeconomics  (Head TA)  Spring 2015 - Economics 101 - Principles of Microeconomics \n Fall 2014 - Economics 101 - Principles of Macroeconomics   ","tags":null,"title":"Past Course Materials","type":"course"},{"authors":null,"categories":null,"content":"#sidebar { background: #cccccc; font-family: monospace }  Professor: Elizabeth Kelly\nLecture: TuesThurs 1:00 - 2:15 PM\nCourse Page\nTA: Gary Baker\nOffice: Soc Sci 6470\nOffice Hours: Mon 11AM-12PM, Wed 2-3PM, or by appointment\nDiscussion Sections\n(315)Thu 3:30-4:20PM - Sterling 2319\n(303)Fri 1:20-2:10PM - Van Hise 386\nExams\nMidterm 1 - Tues, 27 Feb (in class)\nMidterm 2 - Tues, 10 Apr (in class)\nFinal - Sun, 6 May (7:45 - 9:45 AM)\nHomework\nHomework 1 (Due Thurs, 8 Feb)\nHomework 2 (Due Thurs, 22 Feb)\nHomework 3 (Due Thurs, 15 Mar)\nHomework 4 (Due Thurs, 5 Apr)\nHomework 5 (Due Thurs, 3 May)\nDiscussion Handouts\nHandout 1 (Solutions)\nHandout 2 (Solutions)\nHandout 3 (Solutions)\nHandout 4 (Solutions)\nHandout 5 (Solutions)\nHandout 6 (Solutions)\nHandout 7 (Solutions)\nHandout 8 (Solutions)\nHandout 9 (Solutions)\nHandout 10 (Solutions)\nHandout 11 (Solutions)\nHandout 12 (Solutions)\nHandout 13 (Solutions)\nHandout 14 (Solutions)\n Welcome to my Econ 101 page for Spring 2018. Here I will post various updates throughout the semester for my students, as well as provide links to the weekly handouts as they become available. More detailed information on the course can be found on Professor Kelly\u0026rsquo;s webpage.\nUpdates 28 January - I\u0026rsquo;ve uploaded Handout 1 and its solutions. If some of the topics covered in section this week seem a bit unclear, feel free to email me or attend office hours, but also to solve some of the exercises not covered on the handout. Mathematical fluency will come with practice.\n4 February - Handout 2 has been uploaded. Don\u0026rsquo;t forget that the first homework is due this Thursday at the start of lecture.\n18 February - Handouts 3 and 4 uploaded. HW 2 is due this coming Thursday at the start of class, and the first midterm is one week from this Tuesday. More details will be sent via email later this week.\n21 March - Apologies for the delay in updates on account of my recent illness. I\u0026rsquo;ve updated the handouts up to Handout 8.\n24 April - Handouts updated through handout 12. Homework 5 is due Thursday, 3 May. We will not be able to return it before the final exam, so make sure you make a copy if you want it to study by.\n4 May - Final handouts added. Good luck on the exam and have a good break!\n  ","date":1525392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525392000,"objectID":"a9a3d67725354f53c04fedb6ae08ac6a","permalink":"https://www.garygbaker.com/teaching/s2018econ101/","publishdate":"2018-05-04T00:00:00Z","relpermalink":"/teaching/s2018econ101/","section":"teaching","summary":"#sidebar { background: #cccccc; font-family: monospace }  Professor: Elizabeth Kelly\nLecture: TuesThurs 1:00 - 2:15 PM\nCourse Page\nTA: Gary Baker\nOffice: Soc Sci 6470\nOffice Hours: Mon 11AM-12PM, Wed 2-3PM, or by appointment\nDiscussion Sections\n(315)Thu 3:30-4:20PM - Sterling 2319\n(303)Fri 1:20-2:10PM - Van Hise 386\nExams\nMidterm 1 - Tues, 27 Feb (in class)\nMidterm 2 - Tues, 10 Apr (in class)\nFinal - Sun, 6 May (7:45 - 9:45 AM)","tags":null,"title":"Economics 101","type":"teaching"},{"authors":null,"categories":null,"content":"#sidebar { background: #cccccc; font-family: monospace }  Professor: Elizabeth Kelly\nLecture: TuesThurs 8:30-9:40AM\nCourse Page\nTA: Gary Baker\nOffice: Soc Sci 6470\nOffice Hours: Mon/Fri 11AM-12PM, or by appointment\nDiscussion Sections\n(315)Thu 3:30-4:20PM - Sterling 2319\n(303)Fri 1:20-2:10PM - Van Hise 386\nExams\nMidterm 1 - Thurs, 12 Oct (in class)\nMidterm 2 - Thurs, 16 Nov (in class)\nFinal - Tues, 19 Dec (12:25-2:25 PM)\nHomework\nHomework 1 (Due Tues, 3 Oct)\nHomework 2 (Due Tues, 10 Oct)\nHomework 3 (Due Tues, 31 Oct)\nHomework 4 (Due Tues, 14 Nov)\nHomework 5 (Due Tues, 12 Dec)\nDiscussion Handouts\nHandout 1 (Solutions)\nHandout 2 (Solutions)\nHandout 3 (Solutions)\nHandout 4 (Solutions)\nHandout 5 (Solutions)\nHandout 6 (Solutions)\nHandout 7 (Solutions)\nHandout 8 (Solutions)\nHandout 9 (Solutions)\nHandout 10 (Solutions)\nHandout 11 (Solutions)\nHandout 12 (Solutions)\n Welcome to my Econ 101 page for Fall 2017. Here I will post various updates throughout the semester for my students, as well as provide links to the weekly handouts as they become available. More detailed information on the course can be found on Professor Kelly\u0026rsquo;s webpage.\nUpdates 12 December - Handout 12 posted. Good luck on the final exam, and have a good break!\n5 December - Handout 11 posted.\n19 November - Handouts 9 and 10 posted. Exam grades should be posted to Canvas some time this week. Have a good Thanksgiving break!\n6 November - Handout 8 posted. HW 3 has been graded and can be collected from me during office hours or section.\n30 October - Handout 7 posted. HW 3 due tomorrow. As a reminder, some of the harder topics of the course are coming up, so make use of the resources available to you in order not to fall behind.\n25 October - Handout 6 uploaded.\n8 October - Handout 5 uploaded. The first homework has been graded and can be picked up during my office hours on Monday (11 AM - 12 PM and 2 - 3 PM). Don\u0026rsquo;t forget that HW 2 is due Tuesday at the start of lecture, and that if you want to use it to study, to make a copy of it, since we won\u0026rsquo;t be able to return it before the exam.\n1 October - Handout 4 uploaded. The first Midterm will be on Thursday, 12 October. If you are looking to get a headstart on studying, the first place to look should be the past exams on Professor Kelly\u0026rsquo;s webpage, particularly those from past fall and spring semesters.\n25 September - Handout 3 uploaded. As a reminder the homework due date has been pushed to Tuesday, 3 October to allow you more time with the PPF material.\n20 September - Handout 2 has been uploaded. Last section was a lot to take in given that we were a bit ahead of the lecture. Although we will be following the lecture from here on, the pace will not slow down, so it is important to keep up. If anything is confusing, you should work practice problems and ask questions in office hours until you achieve clarity on the topic. Many ideas will build on earlier ones, so unresolved confusion will compound.\n10 September - I\u0026rsquo;ve uploaded Handout 1 and its solutions. If some of the topics covered in section this week seem a bit unclear, feel free to email me or attend office hours, but also to solve some of the exercises not covered on the handout. Mathematical fluency will come with practice.\n  ","date":1513036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513036800,"objectID":"32bdf162e736ce767e59bbfc914b4b5c","permalink":"https://www.garygbaker.com/teaching/f2017econ101/","publishdate":"2017-12-12T00:00:00Z","relpermalink":"/teaching/f2017econ101/","section":"teaching","summary":"#sidebar { background: #cccccc; font-family: monospace }  Professor: Elizabeth Kelly\nLecture: TuesThurs 8:30-9:40AM\nCourse Page\nTA: Gary Baker\nOffice: Soc Sci 6470\nOffice Hours: Mon/Fri 11AM-12PM, or by appointment\nDiscussion Sections\n(315)Thu 3:30-4:20PM - Sterling 2319\n(303)Fri 1:20-2:10PM - Van Hise 386\nExams\nMidterm 1 - Thurs, 12 Oct (in class)\nMidterm 2 - Thurs, 16 Nov (in class)\nFinal - Tues, 19 Dec (12:25-2:25 PM)\nHomework\nHomework 1 (Due Tues, 3 Oct)","tags":null,"title":"Economics 101","type":"teaching"},{"authors":null,"categories":null,"content":"This site was produced using the Hugo static site generator with the Academic theme by George Cushen.\nText and headings are set in Huerta Tipográfica Alegreya and Alegreya Sans respectively. Code is set in Colophon Space Mono. All typefaces are provided by Google Fonts.\nHosting is provided by Github Pages. Image Attribution  Copyright © Randall Munroe. Used according to a CC BY-NC 2.5 License.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"701c5561c1821ccee67ec3fbe2d83f52","permalink":"https://www.garygbaker.com/colophon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/colophon/","section":"","summary":"This site was produced using the Hugo static site generator with the Academic theme by George Cushen.\nText and headings are set in Huerta Tipográfica Alegreya and Alegreya Sans respectively. Code is set in Colophon Space Mono. All typefaces are provided by Google Fonts.\nHosting is provided by Github Pages. Image Attribution  Copyright © Randall Munroe. Used according to a CC BY-NC 2.5 License.\n ","tags":null,"title":"Colophon","type":"page"}]