<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Gary Baker">

  
  
  
    
  
  <meta name="description" content="My job market paper considers the substitutability between different information sources at large sample size&mdash;that is, when information is sufficiently cheap and/or budgets sufficiently large. This file illustrates the main results using Python.
If you&rsquo;d like to experiment yourself, the original Jupyter notebook can be downloaded here (Right click -&gt; &ldquo;Save Link As&rdquo;).
Code setup First, we need to import a number of standard Python packages:
import numpy as np # Basic array stuff import scipy.">

  
  <link rel="alternate" hreflang="en-us" href="https://www.garygbaker.com/web-appendices/info-consumer-theory/">

  


  
  
  
  <meta name="theme-color" content="#c5050c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:400,500%7CAlegreya+Sans:400,500%7CAlegreya:400,500i,700%7CSpace+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://www.garygbaker.com/web-appendices/info-consumer-theory/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Gary Baker">
  <meta property="og:url" content="https://www.garygbaker.com/web-appendices/info-consumer-theory/">
  <meta property="og:title" content="Consumer theory for cheap information - Simulations | Gary Baker">
  <meta property="og:description" content="My job market paper considers the substitutability between different information sources at large sample size&mdash;that is, when information is sufficiently cheap and/or budgets sufficiently large. This file illustrates the main results using Python.
If you&rsquo;d like to experiment yourself, the original Jupyter notebook can be downloaded here (Right click -&gt; &ldquo;Save Link As&rdquo;).
Code setup First, we need to import a number of standard Python packages:
import numpy as np # Basic array stuff import scipy."><meta property="og:image" content="https://www.garygbaker.com/img/icon-32.png">
  <meta property="twitter:image" content="https://www.garygbaker.com/img/icon-32.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2017-01-01T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-09-29T00:00:00&#43;00:00">
  

  



  


  


  





  <title>Consumer theory for cheap information - Simulations | Gary Baker</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Gary Baker</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#teaching"><span>Teaching</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        

        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Consumer theory for cheap information - Simulations</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 29, 2020
  </span>
  

  

  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>My job market paper considers the substitutability between different
information sources at large sample size&mdash;that is, when information is
sufficiently cheap and/or budgets sufficiently large. This file
illustrates the main results using Python.</p>
<p>If you&rsquo;d like to experiment yourself, the original Jupyter notebook can
be downloaded <a href="files/info-consumer-theory.ipynb">here</a> (Right click -&gt; &ldquo;Save
Link As&rdquo;).</p>
<h1 id="code-setup">Code setup</h1>
<p>First, we need to import a number of standard Python packages:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np                   <span style="color:#75715e"># Basic array stuff</span>
<span style="color:#f92672">import</span> scipy.optimize <span style="color:#f92672">as</span> optim       <span style="color:#75715e"># For finding function mins</span>
<span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> multinomial  <span style="color:#75715e"># Multinomial probability computation</span>
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt      <span style="color:#75715e"># Plotting</span>
<span style="color:#f92672">from</span> tabulate <span style="color:#f92672">import</span> tabulate        <span style="color:#75715e"># For nicer printing of some data</span>
</code></pre></div><h1 id="model">Model</h1>
<p>A decision maker must take one of finitely many actions $a\in A$ facing
an uncertain state of the world, $\theta$, that is one of finitely many
possible states. She has a state-dependent payoff function,
$u(a,\theta)$ and chooses her action to maximize expected payoff. Her
prior is given by $p\in\Delta\Theta$.</p>
<p>Prior to acting, she may purchase information about the state and update
her prior based on that information. As standard, define an information
as a <em>Blackwell experiment</em>, that is, a collection of state-dependent
distributions, $F(r\ |\ \theta)$ over some realization space, R:</p>
<p>$$
\mathcal{E} \equiv
\{R, \langle F(\cdot\ |\ \theta)\rangle_{\theta\in\Theta}\}
$$</p>
<p>(In a fully formal treatment, the definition would also include a
Ïƒ-algebra. For the purposes of this paper, we can ignore such
measure-theoretic complications).</p>
<p>After observing a realization from an information source, the decision
maker can update with Bayes rule:</p>
<p>$$
p_\theta&rsquo;(r) = \frac{p_\theta f(r\ | \ \theta)}{\sum_{\theta&rsquo;\in\Theta}p_{\theta&rsquo;}f(r\ |\ \theta&rsquo;)}
$$</p>
<p>To avoid trivialities, assume that no realization perfectly rules in or
out any subset of the states, that is, if realization has positive
probability (density) under one state, it must have positive probability
under all states. (In technical terms, assume the $F(\cdot\ |\ \theta)$
are all mutually absolutely continuous so the Radon-Nikodym derivatives,
$dF(\cdot\ |\ \theta&rsquo;)/dF(\cdot\ |\ \theta)$ all exist.) For notational
simplicity in this illustration, I&rsquo;ll assume each state-dependent
distribution has finitely many possible realizations and thus pmf given
by, $f$.</p>
<p>We can the define an <em>amount</em> of information by a number of
conditionally independent samples from such a source.</p>
<p>For illustration, consider a two-state world, $\theta\in\{H,L\}$. An
information source might be a coin that is fairly waited in the $L$
state, and biased 70% to heads in the $H$ state. Then samples from this
source would simply be the number of coin flips. In an experimental
setting, samples would be literal samples under some experimental
design.</p>
<p>The DM has a collection of information sources $\mathcal{E}_i,\ldots,
\mathcal{E}_I$ from each of which she can purchase an arbitrary number
of samples, $\mathbf{n}=[n_i]$, at some cost $[c_i]$ each.</p>
<p>The goal of this paper is to characterize the substitutability of
different information sources under the normal Bayesian (ex ante)
information value&mdash;that is, the expected payoff gain from acting after
observing a realization from information source $\mathcal{E}$:</p>
<p>$$ V(\mathcal{E}) \equiv \sum_{r\in R} \max_a {\sum_\theta p&rsquo;_\theta(r)
u(a,\theta)} f(r) - \max_a {\sum_\theta p_\theta u(a,\theta)} $$
where $f(r)\equiv \sum_\theta p_\theta f(r\ |\ \theta)$ is the
unconditional realization probability for the given source.</p>
<p>Information value is typically a very poorly behaved function, so I
approach the problem with an asymptotic approach using large deviations
methods.</p>
<p>Throughout this notebook, I&rsquo;ll be working with a 3 state decision
problem.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">numstates <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</code></pre></div><h1 id="composite-information-sources-and-information-values">Composite information sources and information values</h1>
<h2 id="defining-information-sources">Defining information sources</h2>
<p>In order to simulate information values, we need a way to define
Blackwell experiments in a way amenable to computation: Define an
information source as a $|\Theta|\times|R|$ matrix, so each row of the
matrix lists the probability of each realization in that state. I will
typically use Q to denote such a matrix.</p>
<p>The following code will specify a pair of experiments that happens to
look nice, but if you&rsquo;re running the code yourself, you can use the
upper lines to create a pair of random experiments.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rand_source</span>(numstates, numrealizations):
    <span style="color:#75715e"># generate a random array with the appropriate dimension</span>
    Q <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(numstates, numrealizations)
    <span style="color:#75715e"># normalize so each row sums to 1</span>
    <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> range(numstates):
        Q[state, :] <span style="color:#f92672">=</span> Q[state, :] <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sum(Q[state, :])
    <span style="color:#66d9ef">return</span> Q


<span style="color:#75715e">#Q1 = rand_source(numstates, 2)</span>
<span style="color:#75715e">#Q2 = rand_source(numstates, 3)</span>
Qperfect <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(numstates)   <span style="color:#75715e"># Perfect information source</span>

<span style="color:#75715e"># The following two make nice plots</span>
Q1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">0.42</span>, <span style="color:#ae81ff">0.58</span>],
               [<span style="color:#ae81ff">0.63</span>, <span style="color:#ae81ff">0.37</span>],
               [<span style="color:#ae81ff">0.03</span>, <span style="color:#ae81ff">0.97</span>]])
Q2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">0.07</span>, <span style="color:#ae81ff">0.18</span>, <span style="color:#ae81ff">0.75</span>],
               [<span style="color:#ae81ff">0.45</span>, <span style="color:#ae81ff">0.19</span>, <span style="color:#ae81ff">0.36</span>],
               [<span style="color:#ae81ff">0.45</span>, <span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.50</span>]])

<span style="color:#75715e"># state labels. Will use for nice output tables</span>
states <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> stateidx <span style="color:#f92672">in</span> range(Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):
    states<span style="color:#f92672">.</span>append([<span style="color:#e6db74">&#34;State {i}&#34;</span><span style="color:#f92672">.</span>format(i<span style="color:#f92672">=</span>stateidx)])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Conditional probability of each realization:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)

<span style="color:#75715e"># Output table and print</span>
table1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(states, Q1, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
realizations1 <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;Q1&#34;</span>]
<span style="color:#66d9ef">for</span> realization <span style="color:#f92672">in</span> range(Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
    realizations1<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;R1 =&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> str(realization))
<span style="color:#66d9ef">print</span>(tabulate(table1, headers<span style="color:#f92672">=</span>realizations1)<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)

table2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(states, Q2, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
realizations2 <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;Q2&#34;</span>]
<span style="color:#66d9ef">for</span> realization <span style="color:#f92672">in</span> range(Q2<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
    realizations2<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;R2 =&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> str(realization))
<span style="color:#66d9ef">print</span>(tabulate(table2, headers<span style="color:#f92672">=</span>realizations2))
</code></pre></div><pre><code>Conditional probability of each realization:

Q1         R1 = 0    R1 = 1
-------  --------  --------
State 0      0.42      0.58
State 1      0.63      0.37
State 2      0.03      0.97

Q2         R2 = 0    R2 = 1    R2 = 2
-------  --------  --------  --------
State 0      0.07      0.18      0.75
State 1      0.45      0.19      0.36
State 2      0.45      0.05      0.5
</code></pre>
<p>Now, we need to be able to quickly compute the matrix for composite
experiments. First, we need to be able to compute the matrix for $n$
i.i.d. samples from 1 experiment.</p>
<p>Since the total number of realizations of each type is a sufficient
statistic for the entire vector of realizations, we can simplify things
by first computing all of the partitions of $n$ with $|R|$ components
(all possible realization sums), the use a multinomial distribution.</p>
<p>The output matrix will be $|\Theta|\times$(number of ways to sum $|R|$
positive integers to add up to $n$)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Compute appropriate partitions (returns a generator)</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">partitions</span>(n, numrealizations):
    <span style="color:#66d9ef">if</span> numrealizations <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        <span style="color:#66d9ef">if</span> n <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">yield</span> (n,)
        <span style="color:#66d9ef">return</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
        <span style="color:#66d9ef">for</span> result <span style="color:#f92672">in</span> partitions(n<span style="color:#f92672">-</span>i, numrealizations<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
            <span style="color:#66d9ef">yield</span> (i,) <span style="color:#f92672">+</span> result


<span style="color:#75715e"># Compute matrix for the n-sample source</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">n_samples</span>(Q, n):
    numstates <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    numrealizations <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    <span style="color:#66d9ef">if</span> n <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:                   <span style="color:#75715e"># return trivial experiment if 0 samples</span>
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>ones((numstates, <span style="color:#ae81ff">1</span>))
    QnT <span style="color:#f92672">=</span> []                     <span style="color:#75715e"># transpose of Qn</span>
    <span style="color:#66d9ef">for</span> outcome <span style="color:#f92672">in</span> partitions(n, numrealizations):
        outcomeprobs <span style="color:#f92672">=</span> []        <span style="color:#75715e"># column of state-dep outcome probs</span>
        <span style="color:#66d9ef">for</span> state_idx <span style="color:#f92672">in</span> range(numstates):
            <span style="color:#75715e"># create a multinomial with the given outcome probs</span>
            multinom <span style="color:#f92672">=</span> multinomial(n, Q[state_idx, :])
            outcomeprobs<span style="color:#f92672">.</span>append(multinom<span style="color:#f92672">.</span>pmf(outcome))
        QnT<span style="color:#f92672">.</span>append(outcomeprobs)
    Qn <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(QnT)<span style="color:#f92672">.</span>T         <span style="color:#75715e"># convert to array and transpose</span>
    <span style="color:#66d9ef">return</span> Qn

n <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
realizations1 <span style="color:#f92672">=</span> [str(n) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; samples of Q1&#34;</span>]
<span style="color:#66d9ef">for</span> outcome <span style="color:#f92672">in</span> partitions(n, Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
    realizations1<span style="color:#f92672">.</span>append(outcome)
table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(states, n_samples(Q1, n), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;conditional probabilities of each sample combination:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
<span style="color:#66d9ef">print</span>(tabulate(table, headers<span style="color:#f92672">=</span>realizations1))
</code></pre></div><pre><code>conditional probabilities of each sample combination:

3 samples of Q1      (0, 3)    (1, 2)    (2, 1)    (3, 0)
-----------------  --------  --------  --------  --------
State 0            0.195112  0.423864  0.306936  0.074088
State 1            0.050653  0.258741  0.440559  0.250047
State 2            0.912673  0.084681  0.002619  2.7e-05
</code></pre>
<p>Second, we need to be able to composite two <em>distinct</em> information
sources. If info source $\mathcal{E}_1$ and $\mathcal{E}_2$ have $|R_1|$
and $|R_2|$ possible realizations respectively, then the composite
source consisting of 1 sample from each has $|R_1|\times|R_2|$
outcomes. We can get a matrix of all possible combination probabilities
by simply by listing out each element of the outer product of the rows
of each matrix:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">composite_source</span>(Q1, Q2):
    numstates <span style="color:#f92672">=</span> Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    numrealizations <span style="color:#f92672">=</span> Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> Q2<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    Qcomp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>empty((numstates, numrealizations))  <span style="color:#75715e"># initialize output</span>
    <span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> range(numstates):
        <span style="color:#75715e"># compute all possible combination probs with an outer product</span>
        Qcomp[state, :] <span style="color:#f92672">=</span> \
            np<span style="color:#f92672">.</span>reshape(np<span style="color:#f92672">.</span>outer(Q1[state, :], Q2[state, :]),
                       (numrealizations))           <span style="color:#75715e"># reshape to vect.</span>
    <span style="color:#66d9ef">return</span> Qcomp


Q12comp <span style="color:#f92672">=</span> composite_source(Q1, Q2)
realizations12 <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;1 from each&#34;</span>]
<span style="color:#66d9ef">for</span> r1idx <span style="color:#f92672">in</span> range(Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
    <span style="color:#66d9ef">for</span> r2idx <span style="color:#f92672">in</span> range(Q2<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):
        realizations12<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;R1={i1}, R2={i2}&#34;</span><span style="color:#f92672">.</span>format(i1<span style="color:#f92672">=</span>r1idx, i2<span style="color:#f92672">=</span>r2idx))
table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(states, Q12comp, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Conditional probabilties of each combination of realizations from Q1 and Q2:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
<span style="color:#66d9ef">print</span>(tabulate(table, headers<span style="color:#f92672">=</span>realizations12))
</code></pre></div><pre><code>Conditional probabilties of each combination of realizations from Q1 and Q2:

1 from each      R1=0, R2=0    R1=0, R2=1    R1=0, R2=2    R1=1, R2=0    R1=1, R2=1    R1=1, R2=2
-------------  ------------  ------------  ------------  ------------  ------------  ------------
State 0              0.0294        0.0756        0.315         0.0406        0.1044        0.435
State 1              0.2835        0.1197        0.2268        0.1665        0.0703        0.1332
State 2              0.0135        0.0015        0.015         0.4365        0.0485        0.485
</code></pre>
<p>Note that we repeated composite a matrix with itself to get an
equivalent $n$ sample matrix, but this would produce a massive matrix
(size $|R|^n$). Most computers would hit memory limitations for any
$n$ bigger than 20 or so brute forcing it like that isn&rsquo;t practical.</p>
<h2 id="value-of-information">Value of information</h2>
<p>In order to compute information value, we must now define a
state-dependent utility function and a prior belief. I&rsquo;ll code the
utility function as a $|A|\times|\Theta|$ matrix of payoffs where
$U_{a\theta}=u(a,\theta)$. The prior can simply be coded as a vector of
belief probabilities.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># example payoff matrix (payoff 1 only if choose the correct state)</span>
U <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(numstates)
<span style="color:#75715e"># example prior vector (diffuse prior)</span>
P <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(numstates) <span style="color:#f92672">/</span> numstates
</code></pre></div><p>Information value is typically a fairly tricky thing to compute. In
order to maximize computational efficiency, I vectorize the problem
where possible. For a given information matrix, $Q$, and payoff matrix
$U$, we can write the value <em>with</em> information as</p>
<p>$$ W(Q) = \max_D{\text{tr}(QDU\pi)}$$</p>
<p>where $\pi$ is a matrix who&rsquo;s diagonal elements are the prior
probabilities and $D$ is a $|R|\times|A|$ matrix specifying the
probability of taking each action after each realization (this is a
linear program: $D$ generically is all zeros and ones since each
realization generically has a unique optimal response.</p>
<p>(<a href="https://cpb-us-w2.wpmucdn.com/campuspress.yale.edu/dist/3/352/files/2013/01/LeshnoSpector92.pdf">Leshno,
1992</a>
uses this formulation to provide an elementary proof of Blackwell&rsquo;s
theorem for the finite-action/finite-state case.)</p>
<p>The value <em>of</em> information would then be $W(Q)$ minus the payoff from
acting with no information. Such a subtraction is a monotone
transformation, so it won&rsquo;t affect the ordinal properties I&rsquo;m interested
in.</p>
<p>In order to evaluate how close a bundle is to perfect information, I
will sometimes use the ratio of the full-information gap (the difference
between the value of a perfect signal and $W(Q)$, relative to the
full-info value. This will be a percentage that approaches zero as the
amount of samples increases.</p>
<p>None of these approximation would be particularly useful if they require
so many samples as to be indistinguishable from a perfect source
anyways. I will thus use this relative info-gap as an ad hoc measure how
useful the approximation is. That is, the relevant approximations are
useful if they are accurate, even when the relative info-gap is large.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">info_value</span>(Q, U, P):
    numrealizations <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]

    Upi <span style="color:#f92672">=</span> U <span style="color:#960050;background-color:#1e0010">@</span> np<span style="color:#f92672">.</span>diag(P)
    <span style="color:#75715e"># compute (actions x realizations) matrix of payoff of each action</span>
    <span style="color:#75715e"># times unconditional prob of each realization</span>
    Ua <span style="color:#f92672">=</span> Upi <span style="color:#960050;background-color:#1e0010">@</span> Q
    <span style="color:#75715e"># choose best action for each message then sum across messages</span>
    valuewithinfo <span style="color:#f92672">=</span> sum(np<span style="color:#f92672">.</span>amax(Ua, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
    <span style="color:#66d9ef">return</span> valuewithinfo

<span style="color:#75715e"># relative info gap</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relative_info_gap</span>(Q, U, P):
    <span style="color:#66d9ef">return</span> (info_value(Qperfect, U, P) <span style="color:#f92672">-</span> info_value(Q, U, P)) <span style="color:#f92672">/</span> info_value(Qperfect, U, P)

<span style="color:#75715e"># value of info for the examples above</span>
<span style="color:#66d9ef">print</span>(tabulate([[<span style="color:#e6db74">&#34;Expected value of acting after observing Q1:&#34;</span>,
                 info_value(Q1, U, P)]]))
</code></pre></div><pre><code>--------------------------------------------  --------
Expected value of acting after observing Q1:  0.533333
--------------------------------------------  --------
</code></pre>
<h1 id="chernoff-precision">Chernoff precision</h1>
<p>In the paper, I show that two information sources are exchangeable with
ratios of respective precision-like indices of each experiment. In order
to define this precision, we must first take a brief detour into large
deviations theory.</p>
<p>I approach the problem of approximating information values by
approximating the probability of a &ldquo;mistake&rdquo; (taking a suboptimal action
in a given state). The normal form of Bayes&rsquo;s rule is a bit messy, so
instead of working with probabilities, I work with log-likelihood
ratios, where Bayes rule becomes a sum:</p>
<p>$$
\log\bigg(\frac{p_{\theta}'(r)}{p_{\theta&rsquo;}'(r)}\bigg) =
\log\bigg(\frac{p_\theta}{p_{\theta&rsquo;}}\bigg) +
\log\bigg(\frac{f(r\ |\ \theta)}{f(r\ |\ \theta)}\bigg)
$$</p>
<p>And, of course, we have no shortage of asymptotic results for
approximating sums of many independent distributions.</p>
<p>For a given pair of states, define the Chernoff index of an experiment,
$\mathcal{E}_1$, as the minimized value of the moment generating
function (MGF) of the distribution of log-likelihood ratios (LLR):</p>
<p>$$
\rho_1 \equiv \min_t \sum_r f(r\ |\ \theta)^t f(r\ |\ \theta&rsquo;)^{1-t}
$$
(Define $\tau_1$ as the minimizer)</p>
<p>Note that the expected value of the distribution of the above mgf is the
negative Kullback-Leibler divergence, $-D(F(\cdot\ |\ \theta&rsquo;)\ ||<br>
F(\cdot\ |\ \theta))$.</p>
<p>Note that because the MGF of an independent sum is the product of MGFs, we
have that $n$ samples from $\mathcal{E}_1$ will have Chernoff index
$\rho_1^n$.</p>
<p>Furthermore, because the minimum of a sum will be bigger than the sum of
minima, we have that the Chernoff index of a composite is more than the
sum of its parts:</p>
<p>$$
\rho_{12} \geq \rho_1\rho_2
$$</p>
<p>Now, we can define the Chernoff <em>precision</em> for a given state pair of a
test by $\beta \equiv -\log(\rho)$. I call this a precision because, for
Gaussian tests, it is, up to a multiplicative constant, the same as
classical precision ($1/\sigma^2$).</p>
<p>This measure has a number of properties that you might expect for
something called a precision</p>
<ol>
<li>For any non-trivial experiment, $\beta&gt;0$</li>
<li>$n$ samples from the same experiment has precision $n\beta$</li>
<li>Blackwell dominant experiments have higher precision</li>
</ol>
<p>Intuitively, you can think of the Chernoff precision as measuring how
well an information source can distinguish between a given pair of
states.</p>
<p>Because the Chernoff number of a composite is weakly higher than the
product of the individual Chernoff numbers, a composite experiment is
weakly less precise, for a given state, than the sum of it&rsquo;s parts.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># the following returns the full list of state-pair precisions</span>
<span style="color:#75715e"># and their respective MGF minimizers</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">precisions</span>(Q):
    numstates <span style="color:#f92672">=</span> Q<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e"># compute the Chernoff index for each state</span>
    betalist <span style="color:#f92672">=</span> []
    taulist <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> state1 <span style="color:#f92672">in</span> range(numstates):
        <span style="color:#66d9ef">for</span> state2 <span style="color:#f92672">in</span> range(state1<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, numstates):
            <span style="color:#75715e"># Define the llr mgf for the given dichtomy</span>
            <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">llrmgf</span>(t):
                Qstate1t <span style="color:#f92672">=</span> Q[state1, :]<span style="color:#f92672">**</span>t
                Qstate2t <span style="color:#f92672">=</span> Q[state2, :]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>t)
                <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sum(Qstate1t <span style="color:#f92672">*</span> Qstate2t)
            <span style="color:#75715e"># Compute index for the dichotomy</span>
            optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>minimize(llrmgf, <span style="color:#ae81ff">0.5</span>)
            rho <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>fun
            tau <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>x[<span style="color:#ae81ff">0</span>]
            betalist<span style="color:#f92672">.</span>append(<span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>log(rho))
            taulist<span style="color:#f92672">.</span>append(tau)
    <span style="color:#66d9ef">return</span> betalist, taulist


beta1list, tau1list <span style="color:#f92672">=</span> precisions(Q1)
beta2list, tau2list <span style="color:#f92672">=</span> precisions(Q2)
beta12list, tau12list <span style="color:#f92672">=</span> precisions(composite_source(Q1, Q2))
table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([beta12list, [beta1list[i]<span style="color:#f92672">+</span>beta2list[i]
                               <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(beta1list))]])<span style="color:#f92672">.</span>T
<span style="color:#66d9ef">print</span>(tabulate(table,
               headers<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;composite precisions&#34;</span>, <span style="color:#e6db74">&#34;sum of parts&#34;</span>]))
</code></pre></div><pre><code>  composite precisions    sum of parts
----------------------  --------------
              0.149026        0.149196
              0.271436        0.276094
              0.343391        0.343559
</code></pre>
<p>Now, it might seem that composites are always worse than the sum of
their parts since, for any state pair, the composite is always less
precise than the sum of its parts. But Moscarini and Smith (2002) showed
that, for large sample sizes, the only state pair that matters is the
pair hardest to tell apart&mdash;i.e. the pair with the least precision
(highest Chernoff index).</p>
<p>Thus complementarity often arises when experiments differ in the pair of
states they most struggle to distinguish:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">beta1 <span style="color:#f92672">=</span> min(beta1list)
beta2 <span style="color:#f92672">=</span> min(beta2list)
beta12 <span style="color:#f92672">=</span> min(beta12list)
<span style="color:#66d9ef">print</span>(tabulate([[beta12, beta1<span style="color:#f92672">+</span>beta2]],
               headers<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;min precision of composite&#34;</span>, <span style="color:#e6db74">&#34;sum of min precisions&#34;</span>]))
</code></pre></div><pre><code>  min precision of composite    sum of min precisions
----------------------------  -----------------------
                    0.149026                 0.051485
</code></pre>
<p>In this particular example, we can see that it is in fact the composite
has a <em>higher</em> least precision than the sum of least precisions of its
parts.</p>
<h1 id="plotting-indifference-curves">Plotting &ldquo;indifference curves&rdquo;</h1>
<p>In particular, at large samples, two bundles of samples will perform
equally if they have equal least precision. Information value at large
samples is ordinally equivalent to</p>
<p>$$
v(n_1, n_2) \simeq (n_1+n_2)\beta_\omega
$$</p>
<p>where $\beta_\omega$ is the least precision dichotomy for a bundle that
is composed of $\omega$ fraction of samples from $n_1$. Furthermore, we
can breakdown $\beta_\omega$ into component precisions:</p>
<p>$$
\beta_\omega=\omega\beta_{\omega 1}+(1-\omega)\beta_{\omega 2}
$$</p>
<p>where $\beta_{\omega i}$ is $-\log M_i(\tau_\omega)$, is the negative
log of the LLR MGF for the composite&rsquo;s worst-case state pair, evaluated
at the composite&rsquo;s minimizer. We can then write</p>
<p>$$
v(n_1, n_2) \simeq n_1\beta_{\omega 1} + n_2\beta_{\omega 2}
$$</p>
<p>Heuristically, then it seems like the MRS between two samples at any
bundle with $\omega$ fraction from $\mathcal{E}_1$ must then be the
ratio of the component precisions. (For small substitutions, relative
to total sample size, the fraction of samples from each source doesn&rsquo;t
change much, so the component precisions don&rsquo;t change much.)</p>
<p>Additionally, since the value is a min of a sums of precisions, there
will be kinks when the least-precision state pair changes.</p>
<p>Of course, samples are fundamentally discrete so there is no MRS. In the
paper, I formally define a notion of asymptotic MRS, which basically
defines the slope of the boundary between upper and lower contour
sets. For the purpose of interpreting things here, it works well enough
to just pretend samples are divisible.</p>
<p>First, note that the component precisions only depend on the fraction of
the bundle from each source (info value is homothetic). First, I compute
the component precision for a given composite factor $\omega$.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">n1start <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># return the component precision for each test at the w composite factor</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">comp_precision</span>(Q1, Q2, w):
    numstates <span style="color:#f92672">=</span> Q1<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e"># loop over pairs of states</span>
    rho <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> state1 <span style="color:#f92672">in</span> range(numstates):
        <span style="color:#66d9ef">for</span> state2 <span style="color:#f92672">in</span> range(state1<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, numstates):
            <span style="color:#75715e"># Define the Hellinger transform for the given dichtomy</span>
            <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">llrmgf1</span>(t):
                Q1state1_t <span style="color:#f92672">=</span> Q1[state1, :]<span style="color:#f92672">**</span>t
                Q1state2_t <span style="color:#f92672">=</span> Q1[state2, :]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>t)
                <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sum(Q1state1_t<span style="color:#f92672">*</span>Q1state2_t)

            <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">llrmgf2</span>(t):
                Q2state1_t <span style="color:#f92672">=</span> Q2[state1, :]<span style="color:#f92672">**</span>t
                Q2state2_t <span style="color:#f92672">=</span> Q2[state2, :]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>t)
                <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sum(Q2state1_t<span style="color:#f92672">*</span>Q2state2_t)

            <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compllrmgf</span>(t):
                <span style="color:#66d9ef">return</span> llrmgf1(t)<span style="color:#f92672">**</span>w <span style="color:#f92672">*</span> llrmgf2(t)<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>w)

            <span style="color:#75715e"># Compute Chernoff for the dichotomy</span>
            optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>minimize(compllrmgf, <span style="color:#ae81ff">0.5</span>)
            rhopair <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>fun
            taupair <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>x
            <span style="color:#75715e"># if new rho is worse (higher), store it</span>
            <span style="color:#66d9ef">if</span> rho <span style="color:#f92672">&lt;</span> rhopair:
                rho <span style="color:#f92672">=</span> rhopair
                <span style="color:#75715e"># Store component rhos</span>
                rho1w <span style="color:#f92672">=</span> llrmgf1(taupair)
                rho2w <span style="color:#f92672">=</span> llrmgf2(taupair)
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>log(rho1w), <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>log(rho2w)

<span style="color:#75715e"># return total precision for a composite factor w</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">total_precision</span>(Q1, Q2, w):
    beta1w, beta2w <span style="color:#f92672">=</span> comp_precision(Q1, Q2, w)
    <span style="color:#66d9ef">return</span> w<span style="color:#f92672">*</span>beta1w <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>w)<span style="color:#f92672">*</span>beta2w
</code></pre></div><p>We can then compute points on an indifference curve by using the
differential equation defined by the asymptotic MRS ($dn_2/dn_1$):</p>
<p>$$
\text{AMRS}(\omega) = \frac{\beta_{\omega 1}}{\beta_{\omega 2}}
$$</p>
<p>In all the plots that follow, the reference point is the lower right
corner bundle consisting entirely of samples from $\mathcal{E}_1$.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mrs_approx</span>(n1start):
    dn <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
    n1pointsapprox <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(n1start, <span style="color:#f92672">-</span>dn, <span style="color:#f92672">-</span>dn)
    n2pointsapprox <span style="color:#f92672">=</span> []
    n2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> n1 <span style="color:#f92672">in</span> n1pointsapprox:
        w <span style="color:#f92672">=</span> n1 <span style="color:#f92672">/</span> (n1 <span style="color:#f92672">+</span> n2)
        beta1w, beta2w <span style="color:#f92672">=</span> comp_precision(Q1, Q2, w)
        n2 <span style="color:#f92672">=</span> n2 <span style="color:#f92672">+</span> dn<span style="color:#f92672">*</span>(beta1w<span style="color:#f92672">/</span>beta2w)
        n2pointsapprox<span style="color:#f92672">.</span>append(n2)
    <span style="color:#66d9ef">return</span> n1pointsapprox, n2pointsapprox


n1pointsapprox, n2pointsapprox <span style="color:#f92672">=</span> mrs_approx(n1start)
</code></pre></div><p>Now we can compare this to the true upper/lower contour set computed
numerically using the info value function defined earlier. The plot
below shows the locus of bundles that are minimally better than the
reference point (the maximal boundary for the upper contour set).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mrs_true</span>(n1start):
    Q1n <span style="color:#f92672">=</span> n_samples(Q1, n1start)
    startval <span style="color:#f92672">=</span> info_value(Q1n, U, P)
    <span style="color:#75715e"># Trace out the lower extent of the UCS</span>
    n2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>      <span style="color:#75715e"># start with no samples from Q2</span>
    n1pointstrue <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(n1start, <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    n2pointstrue <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> n1loss <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n1start<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
        Q1n <span style="color:#f92672">=</span> n_samples(Q1, n1start<span style="color:#f92672">-</span>n1loss)
        <span style="color:#75715e"># find minimum samples from Q2 to make better off</span>
        currentval <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">while</span> currentval <span style="color:#f92672">&lt;</span> startval:
            n2 <span style="color:#f92672">=</span> n2 <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
            Q2n <span style="color:#f92672">=</span> n_samples(Q2, n2)
            Qcomp <span style="color:#f92672">=</span> composite_source(Q1n, Q2n)
            currentval <span style="color:#f92672">=</span> info_value(Qcomp, U, P)
        n2pointstrue<span style="color:#f92672">.</span>append(n2)
        n2 <span style="color:#f92672">=</span> n2 <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>     <span style="color:#75715e"># decr. n2 just to be sure later</span>
    <span style="color:#66d9ef">return</span> n1pointstrue, n2pointstrue


n1pointstrue, n2pointstrue <span style="color:#f92672">=</span> mrs_true(n1start)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>), dpi<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)
ax<span style="color:#f92672">.</span>set_xlim((<span style="color:#ae81ff">0</span>, n1start))
ax<span style="color:#f92672">.</span>set_ylim((<span style="color:#ae81ff">0</span>, max(n2pointstrue)))
ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Number of samples from Experiment 1&#39;</span>)
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Number of samples from Experiment 2&#39;</span>)
ax<span style="color:#f92672">.</span>plot(n1pointstrue, n2pointstrue, <span style="color:#e6db74">&#39;k&#39;</span>,
        n1pointsapprox, n2pointsapprox, <span style="color:#e6db74">&#39;--k&#39;</span>);
</code></pre></div><p><img src="./info-consumer-theory_37_0.png" alt="png"></p>
<p>Now compute the relative info gap for the indifference curve plotted
above. Higher relative info-gap implies the approximation is useful even
at small samples.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Q1n <span style="color:#f92672">=</span> n_samples(Q1, n1start)
<span style="color:#66d9ef">print</span>(tabulate([[<span style="color:#e6db74">&#34;Relative info-gap:&#34;</span>,
                 relative_info_gap(Q1n, U, P)]]))
</code></pre></div><pre><code>------------------  ---------
Relative info-gap:  0.0438343
------------------  ---------
</code></pre>
<p>Note that the approximation effectively sets the probability of a
mistake other than the most likely one to zero. This approximation then
tends to <em>overestimate</em> payoffs, and thus the approximate MRS will tend
to lie to left of the truth.</p>
<p>In the above plot, we can see that approximation performs relatively
well, even at small sample sizes. One limitation is that the
approximation will always perform somewhat poorly in a region around a
kink for two reasons:</p>
<ol>
<li>The second lowest precision is very close to the lowest, so only
accounting for the lowest precision doesn&rsquo;t work as well; and,</li>
<li>Because the kinks are inward pointing, total sample size tends to be
lower there. In the above example, the corners have total sample size
between 40 and 50, but the kink has only about 15 total samples.</li>
</ol>
<p>But regardless, as sample size increases, we can always get an
arbitrarily good approximation for composite factors arbitrarily close
to that of any kink point.</p>
<p>Below I plot again, but at twice the sample size to illustrate the
convergence:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">n1start <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
n1pointsapprox, n2pointsapprox <span style="color:#f92672">=</span> mrs_approx(n1start)
n1pointstrue, n2pointstrue <span style="color:#f92672">=</span> mrs_true(n1start)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span>), dpi<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)
ax<span style="color:#f92672">.</span>set_xlim((<span style="color:#ae81ff">0</span>, n1start))
ax<span style="color:#f92672">.</span>set_ylim((<span style="color:#ae81ff">0</span>, max(n2pointstrue)))
ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Number of samples from Experiment 1&#39;</span>)
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Number of samples from Experiment 2&#39;</span>)
ax<span style="color:#f92672">.</span>plot(n1pointstrue, n2pointstrue, <span style="color:#e6db74">&#39;k&#39;</span>,
        n1pointsapprox, n2pointsapprox, <span style="color:#e6db74">&#39;--k&#39;</span>);
</code></pre></div><p><img src="./info-consumer-theory_43_0.png" alt="png"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Q1n <span style="color:#f92672">=</span> n_samples(Q1, n1start)
<span style="color:#66d9ef">print</span>(tabulate([[<span style="color:#e6db74">&#34;Relative info-gap:&#34;</span>,
                 relative_info_gap(Q1n, U, P)]]))
</code></pre></div><pre><code>------------------  --------
Relative info-gap:  0.011011
------------------  --------
</code></pre>

    </div>

    


















  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_huadb89bbecdae49a075d475aaac560a84_4843386_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://www.garygbaker.com/">Gary Baker</a></h5>
      
      <p class="card-text">I&rsquo;m an economic theory grad student at UW&ndash;Madison. I am currently (Fall 2020) on the job market.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/ggbaker" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.3.1/mermaid.min.js" integrity="sha256-vOIuDSYDirTfyr+S2MjFnhOz6Rgiz4ODFAHATG0rFxw=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.368ad8701ddf69c34950016f6435139b.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
<p class="powered-by">
    Â© 2020 Gary Baker &middot; 
    
    <a href="/colophon/">Colophon</a>
    

    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
